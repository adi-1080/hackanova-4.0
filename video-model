{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"Meso-4.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6945939,"sourceType":"datasetVersion","datasetId":3989137},{"sourceId":10818426,"sourceType":"datasetVersion","datasetId":6716974},{"sourceId":10818467,"sourceType":"datasetVersion","datasetId":6716999},{"sourceId":10818477,"sourceType":"datasetVersion","datasetId":6717008},{"sourceId":139211,"sourceType":"modelInstanceVersion","modelInstanceId":117881,"modelId":141117},{"sourceId":139339,"sourceType":"modelInstanceVersion","modelInstanceId":117997,"modelId":141233},{"sourceId":139614,"sourceType":"modelInstanceVersion","modelInstanceId":116854,"modelId":140072},{"sourceId":167029,"sourceType":"modelInstanceVersion","modelInstanceId":142122,"modelId":164709},{"sourceId":167046,"sourceType":"modelInstanceVersion","modelInstanceId":142136,"modelId":164721}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/adi1080/my-project?scriptVersionId=224588580\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"45+4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:41:24.282167Z","iopub.execute_input":"2025-02-26T10:41:24.282444Z","iopub.status.idle":"2025-02-26T10:41:24.287832Z","shell.execute_reply.started":"2025-02-26T10:41:24.282424Z","shell.execute_reply":"2025-02-26T10:41:24.286908Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"49"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport cv2\nimport numpy as np\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.io import read_video\nimport random\n\n# Define Constants\nIMG_SIZE = 224\nFRAMES_PER_VIDEO = 10  # Extract 10 frames per video\nBATCH_SIZE = 8\nEPOCHS = 10\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data Transformations\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])\n])\n\n# Frame Extraction Function\ndef extract_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n    frames, _, _ = read_video(video_path, pts_unit='sec')\n    frames = frames.permute(0, 3, 1, 2).float() / 255.0  # Normalize\n    if len(frames) < num_frames:\n        return None  # Skip short videos\n    frame_indices = np.linspace(0, len(frames) - 1, num_frames, dtype=int)\n    selected_frames = frames[frame_indices]\n    return selected_frames\n\n# Custom Dataset Class\nclass DeepFakeDataset(Dataset):\n    def __init__(self, video_paths, labels, transform=None):\n        self.video_paths = video_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        video_path = self.video_paths[idx]\n        label = self.labels[idx]\n        frames = extract_frames(video_path)\n        if frames is None:\n            return self.__getitem__(random.randint(0, len(self.video_paths) - 1))  # Skip empty samples\n        if self.transform:\n            frames = torch.stack([self.transform(frame) for frame in frames])\n        return frames, torch.tensor(label, dtype=torch.long)\n\n# Load Dataset (Assuming Folder Structure: dataset/real/ & dataset/fake/)\nreal_videos = [\"/kaggle/input/1000-videos-split/1000_videos/train/fake\" + f for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/train/fake\")]\nfake_videos = [\"/kaggle/input/1000-videos-split/1000_videos/train/real\" + f for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/train/real\")]\n\nvideo_paths = real_videos + fake_videos\nlabels = [0] * len(real_videos) + [1] * len(fake_videos)  # 0 = Real, 1 = Fake\n\n# Shuffle Data\ncombined = list(zip(video_paths, labels))\nrandom.shuffle(combined)\nvideo_paths, labels = zip(*combined)\n\n# Train-Test Split\nsplit = int(0.8 * len(video_paths))\ntrain_videos, test_videos = video_paths[:split], video_paths[split:]\ntrain_labels, test_labels = labels[:split], labels[split:]\n\n# Create Dataloaders\ntrain_dataset = DeepFakeDataset(train_videos, train_labels, transform)\ntest_dataset = DeepFakeDataset(test_videos, test_labels, transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Loaded {len(train_dataset)} training videos and {len(test_dataset)} testing videos.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:41:24.29209Z","iopub.execute_input":"2025-02-26T10:41:24.292293Z","iopub.status.idle":"2025-02-26T10:41:30.511992Z","shell.execute_reply.started":"2025-02-26T10:41:24.292276Z","shell.execute_reply":"2025-02-26T10:41:30.511195Z"}},"outputs":[{"name":"stdout","text":"Loaded 9306 training videos and 2327 testing videos.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%pip install av","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:41:30.513144Z","iopub.execute_input":"2025-02-26T10:41:30.513621Z","iopub.status.idle":"2025-02-26T10:41:36.552301Z","shell.execute_reply.started":"2025-02-26T10:41:30.513587Z","shell.execute_reply":"2025-02-26T10:41:36.55133Z"}},"outputs":[{"name":"stdout","text":"Collecting av\n  Downloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\nDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: av\nSuccessfully installed av-14.2.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import SwinForImageClassification\n\n# Define Model\nclass SwinDeepFakeDetector(nn.Module):\n    def __init__(self, num_classes=2):\n        super(SwinDeepFakeDetector, self).__init__()\n        self.swin = SwinForImageClassification.from_pretrained(\"microsoft/swin-large-patch4-window7-224\")\n        self.fc1 = nn.Linear(1536, 512)  # Swin-large feature size\n        self.fc2 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        batch_size, frames, c, h, w = x.shape\n        x = x.view(batch_size * frames, c, h, w)  # Flatten batch\n        features = self.swin(x).logits  # Extract features\n        features = features.view(batch_size, frames, -1).mean(dim=1)  # Average over frames\n        x = self.relu(self.fc1(features))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# Initialize Model\nmodel = SwinDeepFakeDetector().to(DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:41:36.55447Z","iopub.execute_input":"2025-02-26T10:41:36.554702Z","iopub.status.idle":"2025-02-26T10:42:16.868945Z","shell.execute_reply.started":"2025-02-26T10:41:36.554682Z","shell.execute_reply":"2025-02-26T10:42:16.867625Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/71.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd9a2b361f1f4eb2bbca6c550249403b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/787M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5e56c2a942c4eb1848203c43d206b93"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\ntorch.backends.cudnn.enabled = False  # Disable CuDNN for RNNs to avoid backward error\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\n# Device Configuration\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nEPOCHS = 10\nLEARNING_RATE = 1e-4\nBATCH_SIZE = 8\nFRAMES_PER_VIDEO = 8  # Number of frames to group per sample\nIMAGE_SIZE = (128,128)  # Resize images\n\n# Transformation for images\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(IMAGE_SIZE),\n    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize images\n])\n\n# Custom Dataset for Frame-Based Videos\nclass FrameDataset(Dataset):\n    def __init__(self, root_dir, label):\n        self.root_dir = root_dir\n        self.label = label\n        self.video_groups = self.group_frames()\n\n    def group_frames(self):\n        frame_dict = {}\n        for filename in sorted(os.listdir(self.root_dir)):\n            if filename.endswith(\".png\"):\n                video_id = \"_\".join(filename.split(\"_\")[:-1])  # Extract video ID prefix\n                if video_id not in frame_dict:\n                    frame_dict[video_id] = []\n                frame_dict[video_id].append(os.path.join(self.root_dir, filename))\n        \n        # Keep only videos with enough frames\n        return {k: v for k, v in frame_dict.items() if len(v) >= FRAMES_PER_VIDEO}\n\n    def __len__(self):\n        return len(self.video_groups)\n\n    def __getitem__(self, idx):\n        video_id = list(self.video_groups.keys())[idx]\n        frame_paths = self.video_groups[video_id][:FRAMES_PER_VIDEO]  # Select required frames\n        \n        frames = []\n        for frame_path in frame_paths:\n            img = cv2.imread(frame_path)\n            if img is None:\n                continue  # Skip corrupted images\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img = transform(img)\n            frames.append(img)\n\n        if len(frames) < FRAMES_PER_VIDEO:\n            return self.__getitem__(np.random.randint(0, len(self)))  # Skip if not enough frames\n\n        frames = torch.stack(frames)  # Shape: [FRAMES_PER_VIDEO, C, H, W]\n        return frames, torch.tensor(self.label, dtype=torch.long)\n\n# Load datasets\nreal_dataset = FrameDataset(\"/kaggle/input/1000-videos-split/1000_videos/train/real\", label=0)\nfake_dataset = FrameDataset(\"/kaggle/input/1000-videos-split/1000_videos/train/fake\", label=1)\n\ntrain_dataset = torch.utils.data.ConcatDataset([real_dataset, fake_dataset])\n# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n\n# # Define Model (Example CNN + LSTM for sequence processing)\nclass VideoClassifier(nn.Module):\n    def __init__(self):\n        super(VideoClassifier, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2)\n        )\n        # Correct input size for LSTM: 128 * 32 * 32 = 131072\n        self.lstm = nn.LSTM(128 * 32 * 32, 256, batch_first=True)\n        self.fc = nn.Linear(256, 2)  # Output: Real or Fake\n\n    def forward(self, x):\n        B, T, C, H, W = x.shape\n        x = x.view(B * T, C, H, W)  # Combine batch and time dimensions\n        x = self.conv(x)  # Apply convolutional layers\n        x = x.view(B, T, -1)  # Reshape to [Batch, Time, Features]\n        _, (hn, _) = self.lstm(x)  # Pass through LSTM\n        out = self.fc(hn[-1])  # Take last LSTM output\n        return out\n\n# Model, Loss & Optimizer\nmodel = VideoClassifier().to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# Training Function\ndef train_model(model, train_loader, optimizer, criterion, epochs=EPOCHS):\n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0\n        correct = 0\n        total = 0\n        loop = tqdm(train_loader, leave=True)\n\n        for videos, labels in loop:\n            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n\n            optimizer.zero_grad()\n            outputs = model(videos)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n            loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n\n        print(f\"Epoch {epoch+1}: Loss = {epoch_loss / len(train_loader):.4f}, Accuracy = {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:42:16.871121Z","iopub.execute_input":"2025-02-26T10:42:16.871696Z","iopub.status.idle":"2025-02-26T10:42:18.184056Z","shell.execute_reply.started":"2025-02-26T10:42:16.871668Z","shell.execute_reply":"2025-02-26T10:42:18.183139Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load datasets\ntrain_real_dataset = FrameDataset(\"/kaggle/input/1000-videos-split/1000_videos/train/real\", label=0)\ntrain_fake_dataset = FrameDataset(\"/kaggle/input/1000-videos-split/1000_videos/train/fake\", label=1)\n\nval_real_dataset = FrameDataset(\"/kaggle/input/1000-videos-split/1000_videos/validation/real\", label=0)\nval_fake_dataset = FrameDataset(\"/kaggle/input/1000-videos-split/1000_videos/validation/fake\", label=1)\n\n# Combine datasets\ntrain_dataset = torch.utils.data.ConcatDataset([train_real_dataset, train_fake_dataset])\nval_dataset = torch.utils.data.ConcatDataset([val_real_dataset, val_fake_dataset])\n\n# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:42:18.185091Z","iopub.execute_input":"2025-02-26T10:42:18.185339Z","iopub.status.idle":"2025-02-26T10:42:18.327252Z","shell.execute_reply.started":"2025-02-26T10:42:18.185317Z","shell.execute_reply":"2025-02-26T10:42:18.326308Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Train Model\ntrain_model(model, train_loader, optimizer, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:42:18.328266Z","iopub.execute_input":"2025-02-26T10:42:18.328564Z","iopub.status.idle":"2025-02-26T10:45:07.547961Z","shell.execute_reply.started":"2025-02-26T10:42:18.32854Z","shell.execute_reply":"2025-02-26T10:45:07.546819Z"}},"outputs":[{"name":"stderr","text":"Epoch [1/10]: 100%|██████████| 93/93 [00:19<00:00,  4.70it/s, acc=56.1, loss=0.674]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss = 0.6882, Accuracy = 56.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/10]: 100%|██████████| 93/93 [00:16<00:00,  5.58it/s, acc=64.2, loss=1.38] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss = 0.6333, Accuracy = 64.19%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=73.8, loss=0.502]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss = 0.5481, Accuracy = 73.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/10]: 100%|██████████| 93/93 [00:16<00:00,  5.61it/s, acc=80.3, loss=0.477]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss = 0.4592, Accuracy = 80.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/10]: 100%|██████████| 93/93 [00:16<00:00,  5.61it/s, acc=83, loss=0.298]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss = 0.4065, Accuracy = 82.97%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/10]: 100%|██████████| 93/93 [00:16<00:00,  5.62it/s, acc=85.3, loss=0.425]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss = 0.3731, Accuracy = 85.27%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=88.5, loss=0.334]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss = 0.3241, Accuracy = 88.51%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/10]: 100%|██████████| 93/93 [00:16<00:00,  5.61it/s, acc=88.8, loss=0.319] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss = 0.3118, Accuracy = 88.78%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=92.3, loss=0.0688]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss = 0.2555, Accuracy = 92.30%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=92, loss=0.624]   ","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss = 0.2333, Accuracy = 92.03%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Training Function\ndef train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0\n        correct = 0\n        total = 0\n        loop = tqdm(train_loader, leave=True)\n        \n        for videos, labels in loop:\n            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n\n            optimizer.zero_grad()\n            outputs = model(videos)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n            loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n\n        train_accuracy = 100 * correct / total\n        train_loss = epoch_loss / len(train_loader)\n\n        # Validation Step\n        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n\n        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.2f}%, \"\n              f\"Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.2f}%, Learning Rate = {optimizer.param_groups[0]['lr']:.6f}\")\n\n# Validation Function\ndef validate_model(model, val_loader, criterion):\n    model.eval()\n    val_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for videos, labels in val_loader:\n            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n            outputs = model(videos)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    val_accuracy = 100 * correct / total\n    return val_loss / len(val_loader), val_accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:45:07.549225Z","iopub.execute_input":"2025-02-26T10:45:07.549617Z","iopub.status.idle":"2025-02-26T10:45:07.558426Z","shell.execute_reply.started":"2025-02-26T10:45:07.549576Z","shell.execute_reply":"2025-02-26T10:45:07.557654Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:45:07.56037Z","iopub.execute_input":"2025-02-26T10:45:07.560619Z","iopub.status.idle":"2025-02-26T10:48:10.651255Z","shell.execute_reply.started":"2025-02-26T10:45:07.560599Z","shell.execute_reply":"2025-02-26T10:48:10.650205Z"}},"outputs":[{"name":"stderr","text":"Epoch [1/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=93.6, loss=0.0927]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 0.2184, Train Accuracy = 93.65%, Val Loss = 0.7204, Val Accuracy = 68.66%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/10]: 100%|██████████| 93/93 [00:16<00:00,  5.61it/s, acc=94.6, loss=0.173] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 0.1793, Train Accuracy = 94.59%, Val Loss = 0.7099, Val Accuracy = 68.66%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/10]: 100%|██████████| 93/93 [00:16<00:00,  5.57it/s, acc=95.4, loss=0.0468]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 0.1594, Train Accuracy = 95.41%, Val Loss = 0.6201, Val Accuracy = 73.88%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/10]: 100%|██████████| 93/93 [00:16<00:00,  5.61it/s, acc=95.5, loss=0.068] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss = 0.1480, Train Accuracy = 95.54%, Val Loss = 0.7220, Val Accuracy = 68.66%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/10]: 100%|██████████| 93/93 [00:16<00:00,  5.61it/s, acc=95.8, loss=0.0971]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss = 0.1306, Train Accuracy = 95.81%, Val Loss = 0.7186, Val Accuracy = 72.39%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/10]: 100%|██████████| 93/93 [00:16<00:00,  5.57it/s, acc=96.5, loss=0.114] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss = 0.1164, Train Accuracy = 96.49%, Val Loss = 0.8288, Val Accuracy = 67.16%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=96.9, loss=0.0554]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss = 0.1075, Train Accuracy = 96.89%, Val Loss = 0.6824, Val Accuracy = 73.13%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=97.8, loss=0.0263]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss = 0.0827, Train Accuracy = 97.84%, Val Loss = 0.6291, Val Accuracy = 75.37%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/10]: 100%|██████████| 93/93 [00:16<00:00,  5.59it/s, acc=98.9, loss=0.0299] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss = 0.0678, Train Accuracy = 98.92%, Val Loss = 0.6765, Val Accuracy = 73.88%, Learning Rate = 0.000100\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/10]: 100%|██████████| 93/93 [00:16<00:00,  5.60it/s, acc=98.9, loss=0.0123] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss = 0.0622, Train Accuracy = 98.92%, Val Loss = 0.7344, Val Accuracy = 75.37%, Learning Rate = 0.000100\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def evaluate_model(model, test_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for videos, labels in tqdm(test_loader):\n            videos, labels = videos.to(DEVICE), labels.to(DEVICE)\n            outputs = model(videos)\n            _, predicted = outputs.max(1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:48:27.971114Z","iopub.execute_input":"2025-02-26T10:48:27.971432Z","iopub.status.idle":"2025-02-26T10:48:27.976469Z","shell.execute_reply.started":"2025-02-26T10:48:27.971406Z","shell.execute_reply":"2025-02-26T10:48:27.975655Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport cv2\nimport os\nfrom tqdm import tqdm\n\n# Load video frames from a directory using OpenCV\ndef load_video_frames(frame_folder, num_frames=16):\n    frames = []\n    \n    # Get all frame image file paths, sorted in correct order\n    frame_paths = sorted([os.path.join(frame_folder, f) for f in os.listdir(frame_folder) if f.endswith(('.png', '.jpg', '.jpeg'))])\n\n    if len(frame_paths) < num_frames:\n        return None  # Skip if not enough frames\n    \n    for frame_path in frame_paths[:num_frames]:  # Limit to `num_frames`\n        frame = cv2.imread(frame_path)  # Read frame\n        if frame is None:\n            continue  # Skip corrupt frames\n\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR → RGB\n        frame = cv2.resize(frame, (224, 224))  # Resize to match model input\n        frames.append(frame)\n\n    if len(frames) < num_frames:\n        return None  # Skip incomplete videos\n\n    frames = np.stack(frames, axis=0)  # Stack into tensor\n    frames = torch.tensor(frames).permute(0, 3, 1, 2).float() / 255.0  # Normalize\n    return frames\n\n# Model evaluation function\ndef evaluate_model(model, test_loader, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            batch_videos = []\n            batch_labels = []\n            \n            for frame_folder, label in batch:\n                frames = load_video_frames(frame_folder)  # Load frames with OpenCV\n                if frames is None:  \n                    continue  # Skip invalid video\n\n                batch_videos.append(frames)\n                batch_labels.append(label)\n\n            if len(batch_videos) == 0:\n                continue  # Skip empty batch\n\n            videos = torch.stack(batch_videos).to(device)\n            labels = torch.tensor(batch_labels).to(device)\n\n            outputs = model(videos)\n            _, predicted = torch.max(outputs, 1)\n\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    \n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:48:30.311008Z","iopub.execute_input":"2025-02-26T10:48:30.311343Z","iopub.status.idle":"2025-02-26T10:48:30.320475Z","shell.execute_reply.started":"2025-02-26T10:48:30.311316Z","shell.execute_reply":"2025-02-26T10:48:30.319453Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# accuracy = evaluate_model(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:48:43.339687Z","iopub.execute_input":"2025-02-26T10:48:43.340042Z","iopub.status.idle":"2025-02-26T10:48:43.343811Z","shell.execute_reply.started":"2025-02-26T10:48:43.340016Z","shell.execute_reply":"2025-02-26T10:48:43.342881Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport cv2\nimport numpy as np\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nfrom tqdm import tqdm\nfrom PIL import Image  # Import PIL for image conversion\n\n# Define Constants\nIMG_SIZE = 224\nBATCH_SIZE = 16\nEPOCHS = 30\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Data Transformations\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for pre-trained models\n])\n\n# Custom Dataset Class for Frame-Based Videos\nclass DeepFakeDataset(Dataset):\n    def __init__(self, frame_paths, labels, transform=None):\n        self.frame_paths = frame_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.frame_paths)\n\n    def __getitem__(self, idx):\n        frame_path = self.frame_paths[idx]\n        label = self.labels[idx]\n        \n        # Load the frame using OpenCV\n        img = cv2.imread(frame_path)\n        if img is None:\n            return self.__getitem__(random.randint(0, len(self.frame_paths) - 1))  # Skip corrupted images\n        \n        # Convert BGR to RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # Convert NumPy array to PIL image\n        img = Image.fromarray(img)\n        \n        # Apply transformations\n        if self.transform:\n            img = self.transform(img)\n        \n        return img, torch.tensor(label, dtype=torch.long)\n\n# Load Dataset (Assuming Flat Structure: dataset/train/fake/ and dataset/train/real/)\ntrain_real_frames = [os.path.join(\"/kaggle/input/1000-videos-split/1000_videos/train/real\", f) for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/train/real\") if f.endswith(('.png', '.jpg', '.jpeg'))]\ntrain_fake_frames = [os.path.join(\"/kaggle/input/1000-videos-split/1000_videos/train/fake\", f) for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/train/fake\") if f.endswith(('.png', '.jpg', '.jpeg'))]\n\nval_real_frames = [os.path.join(\"/kaggle/input/1000-videos-split/1000_videos/validation/real\", f) for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/validation/real\") if f.endswith(('.png', '.jpg', '.jpeg'))]\nval_fake_frames = [os.path.join(\"/kaggle/input/1000-videos-split/1000_videos/validation/fake\", f) for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/validation/fake\") if f.endswith(('.png', '.jpg', '.jpeg'))]\n\ntrain_frames = train_real_frames + train_fake_frames\ntrain_labels = [0] * len(train_real_frames) + [1] * len(train_fake_frames)  # 0 = Real, 1 = Fake\n\nval_frames = val_real_frames + val_fake_frames\nval_labels = [0] * len(val_real_frames) + [1] * len(val_fake_frames)\n\n# Shuffle Data\ncombined_train = list(zip(train_frames, train_labels))\nrandom.shuffle(combined_train)\ntrain_frames, train_labels = zip(*combined_train)\n\ncombined_val = list(zip(val_frames, val_labels))\nrandom.shuffle(combined_val)\nval_frames, val_labels = zip(*combined_val)\n\n# Create Dataloaders\ntrain_dataset = DeepFakeDataset(train_frames, train_labels, transform)\nval_dataset = DeepFakeDataset(val_frames, val_labels, transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Loaded {len(train_dataset)} training frames and {len(val_dataset)} validation frames.\")\n\n# Define Model (ResNet-18)\nclass ResNetDeepFakeDetector(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ResNetDeepFakeDetector, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)  # Load pre-trained ResNet-18\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)  # Modify the final layer for binary classification\n\n    def forward(self, x):\n        return self.resnet(x)\n\n# Initialize Model\nmodel = ResNetDeepFakeDetector().to(DEVICE)\n\n# Loss and Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# Validation Function\ndef validate_model(model, val_loader, criterion):\n    model.eval()  # Set the model to evaluation mode\n    val_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():  # Disable gradient computation\n        for images, labels in val_loader:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = outputs.max(1)  # Get the predicted class\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    val_accuracy = 100 * correct / total\n    return val_loss / len(val_loader), val_accuracy\n\n# Training Function with Validation\ndef train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS):\n    for epoch in range(epochs):\n        model.train()\n        epoch_loss = 0\n        correct = 0\n        total = 0\n        loop = tqdm(train_loader, leave=True)\n        \n        for images, labels in loop:\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            _, predicted = outputs.max(1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            loop.set_description(f\"Epoch [{epoch+1}/{epochs}]\")\n            loop.set_postfix(loss=loss.item(), acc=100 * correct / total)\n\n        train_accuracy = 100 * correct / total\n        train_loss = epoch_loss / len(train_loader)\n\n        # Validation Step\n        val_loss, val_accuracy = validate_model(model, val_loader, criterion)\n\n        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.2f}%, \"\n              f\"Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:49:21.963045Z","iopub.execute_input":"2025-02-26T10:49:21.963344Z","iopub.status.idle":"2025-02-26T10:49:22.224883Z","shell.execute_reply.started":"2025-02-26T10:49:21.963322Z","shell.execute_reply":"2025-02-26T10:49:22.223868Z"}},"outputs":[{"name":"stdout","text":"Loaded 11633 training frames and 2400 validation frames.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"\n# Train Model\ntrain_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T10:49:25.709621Z","iopub.execute_input":"2025-02-26T10:49:25.709998Z","iopub.status.idle":"2025-02-26T11:51:41.635265Z","shell.execute_reply.started":"2025-02-26T10:49:25.709968Z","shell.execute_reply":"2025-02-26T11:51:41.634301Z"}},"outputs":[{"name":"stderr","text":"Epoch [1/30]: 100%|██████████| 728/728 [02:48<00:00,  4.33it/s, acc=92.3, loss=0.872]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 0.1853, Train Accuracy = 92.25%, Val Loss = 0.1551, Val Accuracy = 94.12%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [2/30]: 100%|██████████| 728/728 [01:50<00:00,  6.62it/s, acc=97.2, loss=0.477]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 0.0661, Train Accuracy = 97.23%, Val Loss = 0.2086, Val Accuracy = 93.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [3/30]: 100%|██████████| 728/728 [01:48<00:00,  6.72it/s, acc=97.7, loss=0.994]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 0.0549, Train Accuracy = 97.70%, Val Loss = 0.1875, Val Accuracy = 93.25%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [4/30]: 100%|██████████| 728/728 [01:50<00:00,  6.59it/s, acc=98.5, loss=0.598]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss = 0.0356, Train Accuracy = 98.48%, Val Loss = 0.2495, Val Accuracy = 90.88%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [5/30]: 100%|██████████| 728/728 [01:48<00:00,  6.69it/s, acc=98.5, loss=0.88]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss = 0.0324, Train Accuracy = 98.50%, Val Loss = 0.1360, Val Accuracy = 95.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [6/30]: 100%|██████████| 728/728 [01:48<00:00,  6.73it/s, acc=98.3, loss=0.256]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss = 0.0363, Train Accuracy = 98.29%, Val Loss = 0.1700, Val Accuracy = 93.38%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [7/30]: 100%|██████████| 728/728 [01:47<00:00,  6.74it/s, acc=98.3, loss=0.389]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss = 0.0395, Train Accuracy = 98.32%, Val Loss = 0.2546, Val Accuracy = 91.29%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [8/30]: 100%|██████████| 728/728 [01:47<00:00,  6.77it/s, acc=98.8, loss=0.0299]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss = 0.0257, Train Accuracy = 98.75%, Val Loss = 0.3881, Val Accuracy = 92.88%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [9/30]: 100%|██████████| 728/728 [01:50<00:00,  6.62it/s, acc=98.7, loss=0.0805]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss = 0.0262, Train Accuracy = 98.72%, Val Loss = 0.2877, Val Accuracy = 91.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [10/30]: 100%|██████████| 728/728 [01:48<00:00,  6.72it/s, acc=98.5, loss=0.275]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss = 0.0347, Train Accuracy = 98.52%, Val Loss = 0.1905, Val Accuracy = 93.58%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [11/30]: 100%|██████████| 728/728 [01:48<00:00,  6.69it/s, acc=99.1, loss=0.0899]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss = 0.0145, Train Accuracy = 99.15%, Val Loss = 0.2530, Val Accuracy = 92.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [12/30]: 100%|██████████| 728/728 [01:59<00:00,  6.10it/s, acc=98.7, loss=0.0403]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss = 0.0250, Train Accuracy = 98.69%, Val Loss = 0.1984, Val Accuracy = 94.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [13/30]: 100%|██████████| 728/728 [01:58<00:00,  6.14it/s, acc=98.6, loss=1.09]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss = 0.0282, Train Accuracy = 98.62%, Val Loss = 0.2473, Val Accuracy = 91.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [14/30]: 100%|██████████| 728/728 [01:51<00:00,  6.53it/s, acc=98.9, loss=0.168]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 14: Train Loss = 0.0192, Train Accuracy = 98.92%, Val Loss = 0.1290, Val Accuracy = 95.04%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [15/30]: 100%|██████████| 728/728 [01:49<00:00,  6.66it/s, acc=98.8, loss=0.196]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 15: Train Loss = 0.0262, Train Accuracy = 98.81%, Val Loss = 0.1660, Val Accuracy = 93.38%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [16/30]: 100%|██████████| 728/728 [01:51<00:00,  6.53it/s, acc=99.1, loss=0.173]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 16: Train Loss = 0.0167, Train Accuracy = 99.09%, Val Loss = 0.1360, Val Accuracy = 94.79%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [17/30]: 100%|██████████| 728/728 [01:51<00:00,  6.50it/s, acc=98.7, loss=0.0897]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 17: Train Loss = 0.0280, Train Accuracy = 98.70%, Val Loss = 0.1372, Val Accuracy = 95.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [18/30]: 100%|██████████| 728/728 [01:50<00:00,  6.60it/s, acc=99, loss=0.0307]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 18: Train Loss = 0.0208, Train Accuracy = 98.96%, Val Loss = 0.2135, Val Accuracy = 92.67%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [19/30]: 100%|██████████| 728/728 [01:48<00:00,  6.68it/s, acc=99.2, loss=0.567]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 19: Train Loss = 0.0155, Train Accuracy = 99.17%, Val Loss = 0.1968, Val Accuracy = 94.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [20/30]: 100%|██████████| 728/728 [01:48<00:00,  6.73it/s, acc=99.1, loss=0.0538]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 20: Train Loss = 0.0183, Train Accuracy = 99.06%, Val Loss = 0.2685, Val Accuracy = 91.54%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [21/30]: 100%|██████████| 728/728 [01:48<00:00,  6.73it/s, acc=98.9, loss=0.0123]  \n","output_type":"stream"},{"name":"stdout","text":"Epoch 21: Train Loss = 0.0211, Train Accuracy = 98.93%, Val Loss = 0.2427, Val Accuracy = 91.17%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [22/30]: 100%|██████████| 728/728 [01:48<00:00,  6.72it/s, acc=99.1, loss=1.43]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 22: Train Loss = 0.0169, Train Accuracy = 99.08%, Val Loss = 0.2851, Val Accuracy = 92.83%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [23/30]: 100%|██████████| 728/728 [01:50<00:00,  6.61it/s, acc=98.9, loss=0.396]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 23: Train Loss = 0.0219, Train Accuracy = 98.87%, Val Loss = 0.2092, Val Accuracy = 92.75%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [24/30]: 100%|██████████| 728/728 [01:48<00:00,  6.73it/s, acc=99.3, loss=0.544]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 24: Train Loss = 0.0131, Train Accuracy = 99.29%, Val Loss = 0.1534, Val Accuracy = 95.00%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [25/30]: 100%|██████████| 728/728 [01:47<00:00,  6.77it/s, acc=98.9, loss=0.414]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 25: Train Loss = 0.0252, Train Accuracy = 98.91%, Val Loss = 0.2419, Val Accuracy = 92.42%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [26/30]: 100%|██████████| 728/728 [01:47<00:00,  6.79it/s, acc=99.3, loss=0.18]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 26: Train Loss = 0.0126, Train Accuracy = 99.35%, Val Loss = 0.1959, Val Accuracy = 93.92%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [27/30]: 100%|██████████| 728/728 [01:47<00:00,  6.77it/s, acc=99, loss=1.84]      \n","output_type":"stream"},{"name":"stdout","text":"Epoch 27: Train Loss = 0.0234, Train Accuracy = 99.01%, Val Loss = 0.2896, Val Accuracy = 92.08%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [28/30]: 100%|██████████| 728/728 [01:48<00:00,  6.74it/s, acc=99.2, loss=0.072]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 28: Train Loss = 0.0154, Train Accuracy = 99.18%, Val Loss = 0.1860, Val Accuracy = 94.79%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [29/30]: 100%|██████████| 728/728 [01:47<00:00,  6.75it/s, acc=99.2, loss=0.363]   \n","output_type":"stream"},{"name":"stdout","text":"Epoch 29: Train Loss = 0.0155, Train Accuracy = 99.21%, Val Loss = 0.2147, Val Accuracy = 94.21%\n","output_type":"stream"},{"name":"stderr","text":"Epoch [30/30]: 100%|██████████| 728/728 [01:47<00:00,  6.79it/s, acc=99.1, loss=1.06]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 30: Train Loss = 0.0250, Train Accuracy = 99.08%, Val Loss = 0.6328, Val Accuracy = 84.58%\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Load Test Dataset (Assuming Flat Structure: dataset/test/fake/ and dataset/test/real/)\ntest_real_frames = [os.path.join(\"/kaggle/input/1000-videos-split/1000_videos/test/real\", f) for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/test/real\") if f.endswith(('.png', '.jpg', '.jpeg'))]\ntest_fake_frames = [os.path.join(\"/kaggle/input/1000-videos-split/1000_videos/test/fake\", f) for f in os.listdir(\"/kaggle/input/1000-videos-split/1000_videos/test/fake\") if f.endswith(('.png', '.jpg', '.jpeg'))]\n\ntest_frames = test_real_frames + test_fake_frames\ntest_labels = [0] * len(test_real_frames) + [1] * len(test_fake_frames)  # 0 = Real, 1 = Fake\n\n# Shuffle Test Data\ncombined_test = list(zip(test_frames, test_labels))\nrandom.shuffle(combined_test)\ntest_frames, test_labels = zip(*combined_test)\n\n# Create Test Dataloader\ntest_dataset = DeepFakeDataset(test_frames, test_labels, transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nprint(f\"Loaded {len(test_dataset)} test frames.\")\n\n# Evaluation Function\ndef evaluate_model(model, test_loader):\n    model.eval()  # Set the model to evaluation mode\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():  # Disable gradient computation\n        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            images, labels = images.to(DEVICE), labels.to(DEVICE)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)  # Get the predicted class\n\n            # Collect predictions and labels\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n            # Calculate accuracy\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n    # Calculate overall accuracy\n    accuracy = 100 * correct / total\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n\n    return all_preds, all_labels, accuracy\n\n# Evaluate the Model on the Test Dataset\nall_preds, all_labels, test_accuracy = evaluate_model(model, test_loader)\n\n# Print Confusion Matrix (Optional)\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Compute confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Plot confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T12:12:49.946085Z","iopub.execute_input":"2025-02-26T12:12:49.946421Z","iopub.status.idle":"2025-02-26T12:13:28.354021Z","shell.execute_reply.started":"2025-02-26T12:12:49.946392Z","shell.execute_reply":"2025-02-26T12:13:28.35303Z"}},"outputs":[{"name":"stdout","text":"Loaded 2400 test frames.\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 150/150 [00:37<00:00,  3.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy: 83.58%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x400 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Save the Model\ndef save_model(model, optimizer, epoch, path=\"deepfake_resnet18.pth\"):\n    \"\"\"\n    Save the model's state dictionary, optimizer state, and current epoch.\n    \"\"\"\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n    }, path)\n    print(f\"Model saved to {path}\")\n\n# Train Model\n# We will first train separately and the only save\n# train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS)\n\n# Save the trained model\nsave_model(model, optimizer, EPOCHS, \"deepfake_resnet18.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T08:22:46.36Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load the trained model","metadata":{}},{"cell_type":"code","source":"# Load the saved model\ndef load_model(model, optimizer, path=\"deepfake_resnet18.pth\"):\n    \"\"\"\n    Load the model's state dictionary, optimizer state, and epoch.\n    \"\"\"\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    print(f\"Model loaded from {path} (epoch {epoch})\")\n    return model, optimizer, epoch\n\n# Initialize a new model and optimizer\nloaded_model = ResNetDeepFakeDetector().to(DEVICE)\nloaded_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-4)\n\n# Load the saved model\nloaded_model, loaded_optimizer, epoch = load_model(loaded_model, loaded_optimizer, \"deepfake_resnet18.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T08:22:46.36Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocess the input","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\nfrom PIL import Image\n\n# Define the same transformations used during training\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T08:22:46.36Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predict on a video","metadata":{}},{"cell_type":"code","source":"import cv2\n\ndef predict_video(video_path, model, transform, frame_interval=10):\n    \"\"\"\n    Predict whether each frame in a video is real or fake.\n    \"\"\"\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video {video_path}\")\n        return\n\n    frame_count = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # End of video\n\n        # Process every `frame_interval` frames\n        if frame_count % frame_interval == 0:\n            # Convert the frame to RGB\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_pil = Image.fromarray(frame_rgb)\n\n            # Preprocess the frame\n            frame_tensor = transform(frame_pil).unsqueeze(0).to(DEVICE)\n\n            # Make prediction\n            with torch.no_grad():\n                output = model(frame_tensor)\n                _, predicted = torch.max(output, 1)\n                prediction = \"Fake\" if predicted.item() == 1 else \"Real\"\n\n            # Display the prediction on the frame\n            cv2.putText(frame, f\"Prediction: {prediction}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            cv2.imshow(\"Frame\", frame)\n\n            # Exit if 'q' is pressed\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n        frame_count += 1\n\n    # Release the video capture object and close windows\n    cap.release()\n    cv2.destroyAllWindows()\n\n# Test the video prediction function\nvideo_path = \"/kaggle/input/test-input/bwdmzwhdnw.mp4\"  # Replace with the path to your video\npredict_video(video_path, loaded_model, transform, frame_interval=10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T08:22:46.361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom PIL import Image\n\n# Define Constants\nIMG_SIZE = 128  # Reduce image size\nBATCH_SIZE = 4  # Reduce batch size\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the same transformations used during training\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load the saved model\ndef load_model(model, optimizer, path=\"deepfake_resnet18.pth\"):\n    \"\"\"\n    Load the model's state dictionary, optimizer state, and epoch.\n    \"\"\"\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    print(f\"Model loaded from {path} (epoch {epoch})\")\n    return model, optimizer, epoch\n\n# Initialize a new model and optimizer\nprint(\"Initializing model...\")\nloaded_model = ResNetDeepFakeDetector().to(DEVICE)\nloaded_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-4)\n\n# Load the saved model\nprint(\"Loading model...\")\nloaded_model, loaded_optimizer, epoch = load_model(loaded_model, loaded_optimizer, \"deepfake_resnet18.pth\")\n\n# Predict on a single image\ndef predict(image_path, model, transform):\n    \"\"\"\n    Predict whether an image is real or fake.\n    \"\"\"\n    print(f\"Loading image: {image_path}\")\n    img = Image.open(image_path).convert(\"RGB\")  # Ensure the image is in RGB format\n    print(\"Image loaded.\")\n\n    print(\"Preprocessing image...\")\n    img = transform(img).unsqueeze(0).to(DEVICE)  # Add batch dimension and move to device\n    print(\"Image preprocessed.\")\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Make prediction\n    print(\"Making prediction...\")\n    with torch.no_grad():\n        output = model(img)\n        _, predicted = torch.max(output, 1)\n    \n    return \"Fake\" if predicted.item() == 1 else \"Real\"\n\n# Test the prediction function\nimage_path = \"/kaggle/input/image2/Screenshot 2025-02-22 072822.jpg\"  # Replace with the path to your image\nprint(f\"Predicting on image: {image_path}\")\nprediction = predict(image_path, loaded_model, transform)\nprint(f\"Prediction: {prediction}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T08:22:46.361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\n\n# Define Constants\nIMG_SIZE = 224\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define the same transformations used during training\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Define the model architecture\nclass ResNetDeepFakeDetector(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ResNetDeepFakeDetector, self).__init__()\n        self.resnet = models.resnet18(pretrained=True)\n        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n\n    def forward(self, x):\n        return self.resnet(x)\n\n# Load the saved model\ndef load_model(model, optimizer, path=\"deepfake_resnet18.pth\"):\n    \"\"\"\n    Load the model's state dictionary, optimizer state, and epoch.\n    \"\"\"\n    checkpoint = torch.load(path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    print(f\"Model loaded from {path} (epoch {epoch})\")\n    return model, optimizer, epoch\n\n# Initialize a new model and optimizer\nprint(\"Initializing model...\")\nloaded_model = ResNetDeepFakeDetector().to(DEVICE)\nloaded_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-4)\n\n# Load the saved model\nprint(\"Loading model...\")\nloaded_model, loaded_optimizer, epoch = load_model(loaded_model, loaded_optimizer, \"deepfake_resnet18.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T08:22:46.361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom PIL import Image\n\ndef predict_video(video_path, model, transform, frame_interval=10):\n    \"\"\"\n    Predict whether each frame in a video is real or fake.\n    \"\"\"\n    # Open the video file\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video {video_path}\")\n        return\n\n    frame_count = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # End of video\n\n        # Process every `frame_interval` frames\n        if frame_count % frame_interval == 0:\n            # Convert the frame to RGB\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frame_pil = Image.fromarray(frame_rgb)\n\n            # Preprocess the frame\n            frame_tensor = transform(frame_pil).unsqueeze(0).to(DEVICE)\n\n            # Make prediction\n            with torch.no_grad():\n                output = model(frame_tensor)\n                _, predicted = torch.max(output, 1)\n                prediction = \"Fake\" if predicted.item() == 1 else \"Real\"\n\n            # Display the prediction on the frame\n            cv2.putText(frame, f\"Prediction: {prediction}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n            cv2.imshow(\"Frame\", frame)\n\n            # Exit if 'q' is pressed\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n        frame_count += 1\n\n    # Release the video capture object and close windows\n    cap.release()\n    cv2.destroyAllWindows()\n\n# Test the video prediction function\nvideo_path = \"/kaggle/input/test-input/bwdmzwhdnw.mp4\"  # Replace with the path to your video\npredict_video(video_path, loaded_model, transform, frame_interval=10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T08:22:46.361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}